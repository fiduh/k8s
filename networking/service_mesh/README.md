## What's a Service Mesh & Why do you need it?

Service mesh is a framework(Set of principles for Observability, Security, and Connectivity Pillars) and platform(means it's pluggable, anything can connect in, be it a micoservice or function) where microservices have the ability to discover each other, connect to each other, but also provide authentication and authorization mechanisms, so there's trust and identity build into every single communication stream.

- Service mesh as a framework: Set of principles that we want to align to create resilient microservices, it offers the pillars of Observability, Security, and Connectivity(Traffic management).
  - Service mesh as a platform: Specific implementation of those specific pillars.

  - Service mesh helps with East-West (Services to Service) Traffic management, without making changes to your code or having application developers write those logics inside of the the application.
  - By default services in a kubernetes cluster can talk to each other, so why still need a service mesh? Service mesh adds on capabilities that are not available by default to service to service communication, such as:
    - mTLS(secures the communication between services using certificates, generated by the service mesh)
    - Advanced deployment strategies (Canary/AB/Blue-Green deployments)
    - Observability (Service mesh like Istio integrates well with Kiali(Visualization tool for services in a cluster))
    - Circuit breaking, Traffic splitting, Fault injection, etc.

### Why Istio? Because implementations of service mesh already existed before Istio

- Istio uses an opensource reverse proxy called Envoy which handles connections, it can allow you to enforce policy, it also allows you to provide an observability point and other capabilities that can be highly customized. Istio takes advantage of this very high-powered proxy, but also automate it and the make it available to workloads in a very simplistic manner. So you can use Envoy but through a control plane that abstracts how you would configure Envoy and this primarily because Envoy itself is very complex to configure. Istio solves this particular problem where it will simplify how you would configure envoy but you wouldn't have to directly configure it.

### It's Istio implementation

- In namespaces where Istio is enabled, Istio adds another conatiner(Sidecar container) to the existing contains in every pod.
- The side car container is an envoy proxy server which handles traffic management of the pods. It intercepts traffic going in or out of containers in a pod.
- IstioD is the primary component of Istio called the control plane and runs as Pod in the cluster.
- Anything that you pass as a YAML configuaration that Istio understands, IstioD will then take that configuaration and translate it for Envoys understanding and that becomes a configuartion. This connection, translation and issuance of configuration happens through something called the XDS data plane which is part of envoy. XDS data plane is the mechanism and the lane way for which istio sends configs to Envoy.

### Admission Controllers

- How Istio adds Sidecar containers to every Pod, when a request is sent to the API server for a Pod creation, Istio should immediately be notified, so it can decide if it wants to add a Side car container to it, Istio uses an advanced concept Admission controllers called Dynamic Admission control or Admission webhook to achieve this.
  - A simple request to the API server, for instance a user wants to create a Pod in Kubernetes Cluster (using kubectl apply ....), request goes to the API server, a component in the API server checks if the user is authenticated and authorized to perform this request or not, if the user is authenticated and authorized, then API server will take this object and persist(stores) it in etcd. Admission controllers can intercept(Mutate or Validate) the request before it's persisted in etcd.
  - For example, lets say you want to create a PVC and you didn't add a storageClass in the PVC object, before the resource is added to etcd, there's an admission controller called storageClass admission controller, it will check if the PVC has the storage class field or not, if it doesn't. It will mutate the object and add the field, then the object is persisted in etcd. There are about 30+ admission controllers available by default in every Kubernetes cluster, you don't have to install them, they are precompiled into the API server. sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml . Check the command argument --enablle-admision-plugins
  - How Istio implements addmission controllers, if you look at all the other admission controllers, they are precompilled into the API server, API server clearly knows how to invoke them and what action will those admission controllers take, either mutate or validate an object. In the case of Istio to add a side car controller, Istio should know when a pod creation request comes to the API server and some how API server should notify Istio now you can proceed with the side car injection, the concept is called Dynamic Admission Control.
    - This happens in multiple stages, Pod creation request comes to the API server, API server performs Authentication and Authorization, then there are two special admission controllers "Mutating Admission webhook controller" and "Validation Admission webhook controller". The responsibilities of these controllers is to take requests from the API server and they notify Istio or any other project that wants to implement the side car injection or any kind of mutation and validation, these components do not directly perform the mutation and validation, it only forwards the requests to "ISTIOD Admission webhook(part of ISTIOD)", this ISTIOD webhook then performs the mutation logic(Injects the sidecar container) and object is persisted into etcd.

### Buzz about EBPF, how will it affect service mesh, will it replace it?

EBPF in some ways has augmented and helped the performance of Istio in certain ways, but it's not going to be a replacement in any way.
A Pod is what we call a networking namespace and within that Pod it has a single TCP IP stack, it has an IP address, this is the way the pod can communicate with the rest of the network and other Pods, but if I deploy a Sidecar, I have two containers now running in that Pod and if those two containers have thesame IP, how do we know to distinguish between those two containers? So we use PORT numbers, PORT numbers create some distinction, so one container can call the other container on a given PORT through something called Local Host, so we know who we're trying to communicate with but it'll all be contained within that same Pod or network namespace.
Anytime the main application container needs to make an external call or a call to another service that has to flow through the sidecar container proxy, in fact it has to know how to get to the sidecar proxy and then the sidecar proxy will then route the request to it's destination, in other for that to happen when you actually deploy Istio and you turn on something called sidecar injection and then you deploy a new application into a sidecar enabled namespace, IP tables is rewritten so that traffic now forwards through the proxy. The key here is that traffic has to go from the main application to the sidecar proxy, there might be some latency in that situation. Latency means the request might be slower to respond and that could be problematic.
The way EBPF works is that it optimizes traffic and its movement in the kernel space, so it's able to make decisions very quickly in the kernel space and so you can use EBPF logic to optimize kernel space traffic and optimize the way the flow goes from the main app to the sidecar. Long way to say will EBPF change Istio? yes and no; it won't replace things that happen in Istio but it can provide some optimizations for the way traffic moves between an application container and its sidecar.

When we are talking about the kernel, we are talking about the host machine that all the Pods are sitting on. EBPF is not Pod specific, it's Host Machine specific.

eBPF augments Istio's performance but does not replace it. To understand why, consider how a Pod works: it's a network namespace with a single IP address. When a sidecar container is injected, both containers share that IP and are distinguished by port numbers, communicating via localhost. Any traffic from the main application to an external service must flow through the sidecar proxy — made possible by Istio rewriting iptables rules on deployment so that traffic is redirected through the proxy. This redirection introduces some latency.
eBPF optimizes this by operating in kernel space, where it can make routing decisions much faster. Specifically, it can replace iptables-based traffic redirection with more efficient kernel-level handling, reducing the latency between the application container and its sidecar. What eBPF cannot do is replicate the higher-level functionality of the sidecar itself (traffic management, mTLS, retries, etc.).
The bottom line: eBPF speeds up how traffic gets to the sidecar — it doesn't replace what the sidecar does. It's a host-level (node-level) technology, not Pod-specific, so its optimizations apply across all Pods on a given machine.

### Istio is also conforming to the Gateway API Spec.

- The Gateway API capabilities are present in Istio and of version 1.17, you are actually offered the option to either use the Istio ingressGateway or the Gateway API spec when you're deploying Istio.

### Install Istio

[Install & Configure Istio](https://istio.io/latest/docs/setup/getting-started/)
The preferred production approach to install Istio is using Helm, there're Helm charts available for deploying Istio, you can customize them to your liking, you can even use Helm to customize some aspects of the Istio control plane, just to do things like, I only want to deploy an ingressGateway not and egressGateway or maybe I want to turn on the ability to do DNS filtering. Istio can also be deployed using IstioCTL or Istio Operator, both not recommened for production.

- Key things to get a Mesh up and running:
  - Chose the right profile, because there are several kinds of Istio profiles eg Demo, Default, Ambient, etc. These are different configurations to get started with a certain usecase.

### Enable Sidecar Injection

```bash
kubectl label namespace <namespace-name> istio-injection=enabled

```

### Capabilities of Istio

- Load Balancing
- Service to Service authentication: this can easily be broken down to authenticating your requests, how mTLS functions, how certificates are issued. We can even breakdown the service-service authorization and how Istio enforces policies.
- Monitoring, Tracing and Logging: The reason why we would want to do Monitoring and Logging within our service mesh is really to understand the traffic part of a request, there might be multiple services that are hitting that path before a request provides a response. If there's a failed service we want to know about it, why it failed and how to recover from it. There are capabilities such as tracing that are built into the service mesh, we are able to extract logging information froma sidecar point of view and we're able to pair with tools like Prometheus and Grafana to capture some more Telemetry.
- Service resiliency: It's about tunning your microservices and the way and how quickly they respond, so if we haven't truely implememnted autoscaling, we could be in a situation where one service is overloaded with a number of requests and because of the fact that it's overloaded that could traslate to an over consumption of CPU and memory, when it runs out of CPU and Memory what happens? it cannot do anything, it's like halted at that point. So we have to accomodate for that level of buffering and flow control so that when we send out a request we shouldn't expect a quick response, but if we are, we have to tune the rest of that request path acordingly

- Some pre-requisities for Pods to be part pf Istio Service Mesh:
  - Pods should always run behind one or more service
  - Pods should not run with a security context with user id 1337
  - Pods should run with NET_ADMIN and NET_RAW capabilities
  - Pods/Deployments should have labels "app" and "version"

### Traffic Management

Traffic management is Routing rules

- Components in Traffic Management:
  - Virtual Services
  - Destination Rules
  - Gateways(istioIngressGateway)
  - Service Entries
  - Sidecars

### Virtual services

- Translates a hostname to a kubernetes service, once it finds that match of the kubernetes service by default as long as there's no other policy applied to it, it will forward that request to the label that matches that kubernetes service which happens to be the Pod.

### Destination Rules

### Circult Breaking

### mTLS (Multual TLS)

- The concept of Authentication: normally when we have to authenticate to something we issue a username and password, that's the most common approach, but services that communicate amongst each other are not going to say take my username and password and verify that it's a legitimate username in your database. So the mechanism that is normally preferred is to use client certificates or more specifically transport layer certificates or even more specifically transport layer authentication and encryption, TLS is the short form of it. The simple idea is I'm going to make a request out to you but you have to verify to me that you're authentic and you're legitimate and you're who you say you're and if you are who you say you are then the transaction can go through fine, and that's what we call one way TLS. Now if I communicate with a server and the server comes back to me and says I also want to validate you to see who you are and you truly are who you say you are, once I validated that then we can have bidirectional communication. In Istio there's this concept called peer authentication which enables us to determine if we have sidecare resources that can authenticate to us. If they have a sidecar and a certificate issued and the policy says you're allowed to talk to each other then we can authenticate each other and our communication stream can go through but also that mTLS offers up encryption so the traffic(payloads) that's in motion between these two workloads is encrypted. i.e the traffic between workloads adheres to Mutal authentication and encryption - All we are really doing with this is solving the Identities and solving the payload encryption piece, but there's another piece of authorization(What services can do and what actions can they perform against other services. This ensures that RBAC is in effect).
- The user is going to be responsible for enabling something called **peerAuthentication** which is a custom resource in Istio. This simply enables the authentication as well as the encryption aspects of the service to service communication in security.
- Sperately a user will also have to enable authorization so that when we make HTTP requests between services they can only perform request methods like get versus a delete. This is handled by an Istio resource called **authourizationPolicy**

### Gateways

### Observability

- Within Observability there's this concept that we are trying to align to: Latency, Errors, Traffic and Saturation (LETS) - The Golden signals of Traffic.
- Istio aims to provide these metrics on these four Golden Signals, so you can discern how services are responding, if they are responding quickly enough, if they are returning the right HTTP status codes, if they are met with some latency, if there are failed requests in a given amount of time and that's all important for building the resiliency into our microservices.
- Istio provides you the facility to capture logging, tracing information and even metrics around latency or failed requests or even a given amount of requests in a certain period of time. That data that you take is allowing you to decide how you best tune your environment, but it's also telling of failures, any sort of caling limitations that you're currently experiencing and even tells you about any errors that you might be seeing inside of your mesh configuration. It's a great mechanism to help you troubleshoot aspects of services in your mesh.
- The idea is to make sure we can capture Telemetry somewhere and export it to a system or solution for further analysis and Istio captures that Telemetry for short-term storage, it's not meant for long term storage but there has to be a sync somewhere where all of this can be stored and then further analyzed down the line. It's normally a situation where you pair Istio with something like Prometheus and Grafana in addition to tools like Kali(visualizer of your traffic flow) and Jagger. Grafana gives you a health pespective of how your services and cluster nodes are performing much more form a standpoint of CPU and Memory, Prometheus takes that a little further and gives you more specifics around those details (Think of Prometheus and Grafana as performance related). This adheres to opeTelemetry standards. Jagger is one of those tools that's responsible for telling us how all of our services connect, so when I make a request it's not just one service that responds to it, if we have a bunch of 5 Pods chained together, in Jagger these five Pods will be a part of what we call a request flow, we have to know the sequence of that request flow and how these services are tied together. There's something called B3 header propagation or better known as X header ID, which is a unique ID that ties every single one of those hops together, so it stitches them together so we can visualize, another way to look at this is I am curling to get a response from Service A, Service A talks to Service B, Service B talks to Service C, Service C talks to Service D, before I get my response. Now that B3 header information or X header ID information is consistent between those four Services, signifying that they're tied together for this given request, we call this a Trace, within a Trace we have several spans, each of those hops is considered a Span. This information is very powerful to have if someone is trying to debug.

### Service Mesh vs Ingress

### Istio Sidecarless model called Ambient mesh

- Ambient mesh shifts sidecars to Z tunnel(L4) and Waypoint(L7)
- In Ambient mesh because we don't have a sidecar deployed along side the main application container, we still need something to service that sidecar functionality model. We deploy something called a Z tunnel and a Waypoint proxy. Z tunnel is more common than a waypoint proxy, here's why, in the regular Envoy sidecar proxy it actually does L4 and L7, it handles layer 4 requests, provided policy and even does things at TCP. Now in Ambient mesh we split that functionality into TCP based functionality and UDP based functionality and then the layer 7 stuffs. The reason this was also done is to reduce the overall footprint of the Z tunnel resource and only let it do things like mTLS and layer 4 policy and some observability, but if we want to do layer 7 authorization, any sort of fancy rate limiting or any sort of resiliency or even some authorization policy for like HTTP methods, that's where the Waypoint comes in as needed. Z Tunnel is built in Rust and is a Rust based proxy.

https://www.udemy.com/course/aws-certified-devops-engineer-professional-hands-on/
